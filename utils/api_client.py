# api_client.py
from groq import Groq


def get_algorithm_explanation(algo_type: str, algo_input: str , algo_programing_language: str) -> str:
    """
    Calls the Groq API to generate an explanation for a given algorithm.

    Parameters:
    - algo_type: A brief description of the algorithm type (e.g., "Sorting", "Graph Traversal").
    - algo_input: A detailed description or input for the algorithm.

    Returns:
    - A string containing the explanation returned by the API.
    """
    client = Groq()

    # Build the prompt using the provided algorithm details.
    prompt = (
        f"Explain how an algorithm of type '{algo_type}' works when provided with the following input: {algo_input}. and programing language will be used: {algo_programing_language}.Remember to explain algorithms in a clear and easy-to-understand way for high school students. Also provide how numbers are used in the each step in program with reference to the input like repeating fuction and commenting the number in it   "  )

    chat_completion = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
        model="llama-3.3-70b-versatile",
        temperature=0.5,
        max_completion_tokens=10240,
        top_p=1,
        stop=None,
        stream=False,
    )

    return chat_completion.choices[0].message.content


def get_theory_explanation(topic: str, depth: int, level: int) -> str:
    """
    Calls the Groq API to generate a theory explanation based on the given parameters.

    Parameters:
        topic (str): The theory topic to explain.
        depth (int): How deep the explanation should be.
        level (int): The level of detail desired.
        param2 (int): Additional parameter 2.
        param3 (int): Additional parameter 3.

    Returns:
        str: The explanation generated by the API.
    """
    client = Groq()

    # Construct a prompt that uses all parameters.
    prompt = (
        f"Provide an explanation of the theory on '{topic}'. "
        f"The explanation should be {depth} levels deep and at detail level {level}. "

    )

    chat_completion = client.chat.completions.create(
        messages=[
            {"role": "system", "content": "You are a knowledgeable assistant."},
            {"role": "user", "content": prompt}
        ],
        model="llama-3.3-70b-versatile",
        temperature=0.5,
        max_completion_tokens=1024,
        top_p=1,
        stop=None,
        stream=False,
    )

    return chat_completion.choices[0].message.content